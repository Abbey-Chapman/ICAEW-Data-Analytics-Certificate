{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICAEW Data Analytics Certificate Programme Case Study (Analyst pathway)\n",
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Case Study for the ICAEW Data Analytics Certificate Programme (Analyst pathway). \n",
    "\n",
    "We have discussed the background and context and examined the data and techniques to be used in the case study already. Now we will use the skills we have learnt from this course to analyse the sales of the company we may be investing in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Journals and Sales datasets provided. The structure of this notebook is as follows:\n",
    "\n",
    "- First, we will start off by loading and viewing the datasets.\n",
    "- We will see that the datasets have a mixture of both numerical and non-numerical features.\n",
    "- We will see that the datasets have various data quality issues, such as they contain a number of missing entries and duplicates.\n",
    "- We will use techniques covered in the course to address these issues and prepare the data for analysis.\n",
    "- Finally, we will append and join any relevant datasets together to wrangle our data into a usable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and viewing the datasets\n",
    "\n",
    "First, loading and viewing the datasets. The sales dataset and the journals dataset provided are an Excel, and two csv files respectively. We examined how to load Excel and csv datasets within the course in Module 1 of Unit 2: Data Wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Shipping Cost</th>\n",
       "      <th>Order Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4042</td>\n",
       "      <td>MX-2015-AB1001539-42353</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>AB-1001539</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apopa</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-CH-5379</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Novimex Executive Leather Armchair, Black</td>\n",
       "      <td>610.6000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>238.1200</td>\n",
       "      <td>57.833</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4041</td>\n",
       "      <td>MX-2015-AB1001539-42353</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>AB-1001539</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apopa</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-SU-2966</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>Acme Box Cutter, High Speed</td>\n",
       "      <td>151.2000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.6000</td>\n",
       "      <td>10.786</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24145</td>\n",
       "      <td>IN-2015-AB1001558-42256</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>AB-1001558</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hubli</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-BI-6383</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Wilson Jones Binding Machine, Durable</td>\n",
       "      <td>50.4600</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.6800</td>\n",
       "      <td>10.540</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24144</td>\n",
       "      <td>IN-2015-AB1001558-42256</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>AB-1001558</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hubli</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-BI-3737</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Cardinal Index Tab, Clear</td>\n",
       "      <td>26.8800</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>6.550</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26085</td>\n",
       "      <td>ID-2015-AB1001559-42178</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>2019-06-27</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>AB-1001559</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Palembang</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-FU-3935</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Furnishings</td>\n",
       "      <td>Deflect-O Door Stop, Erganomic</td>\n",
       "      <td>372.9132</td>\n",
       "      <td>12</td>\n",
       "      <td>0.27</td>\n",
       "      <td>101.8332</td>\n",
       "      <td>53.070</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID                 Order ID Order Date  Ship Date       Ship Mode  \\\n",
       "0    4042  MX-2015-AB1001539-42353 2019-12-15 2019-12-19  Standard Class   \n",
       "1    4041  MX-2015-AB1001539-42353 2019-12-15 2019-12-19  Standard Class   \n",
       "2   24145  IN-2015-AB1001558-42256 2019-09-09 2019-09-09        Same Day   \n",
       "3   24144  IN-2015-AB1001558-42256 2019-09-09 2019-09-09        Same Day   \n",
       "4   26085  ID-2015-AB1001559-42178 2019-06-23 2019-06-27    Second Class   \n",
       "\n",
       "  Customer ID  Customer Name   Segment  Postal Code       City  ...  \\\n",
       "0  AB-1001539  Aaron Bergman  Consumer          NaN      Apopa  ...   \n",
       "1  AB-1001539  Aaron Bergman  Consumer          NaN      Apopa  ...   \n",
       "2  AB-1001558  Aaron Bergman  Consumer          NaN      Hubli  ...   \n",
       "3  AB-1001558  Aaron Bergman  Consumer          NaN      Hubli  ...   \n",
       "4  AB-1001559  Aaron Bergman  Consumer          NaN  Palembang  ...   \n",
       "\n",
       "    Product ID         Category Sub-Category  \\\n",
       "0  FUR-CH-5379        Furniture       Chairs   \n",
       "1  OFF-SU-2966  Office Supplies     Supplies   \n",
       "2  OFF-BI-6383  Office Supplies      Binders   \n",
       "3  OFF-BI-3737  Office Supplies      Binders   \n",
       "4  FUR-FU-3935        Furniture  Furnishings   \n",
       "\n",
       "                                Product Name     Sales Quantity Discount  \\\n",
       "0  Novimex Executive Leather Armchair, Black  610.6000        2     0.00   \n",
       "1                Acme Box Cutter, High Speed  151.2000        6     0.00   \n",
       "2      Wilson Jones Binding Machine, Durable   50.4600        1     0.00   \n",
       "3                  Cardinal Index Tab, Clear   26.8800        4     0.00   \n",
       "4             Deflect-O Door Stop, Erganomic  372.9132       12     0.27   \n",
       "\n",
       "     Profit  Shipping Cost  Order Priority  \n",
       "0  238.1200         57.833          Medium  \n",
       "1   75.6000         10.786          Medium  \n",
       "2   22.6800         10.540        Critical  \n",
       "3   12.0000          6.550        Critical  \n",
       "4  101.8332         53.070          Medium  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages required\n",
    "import pandas as pd\n",
    "# Load the sales dataset as sales\n",
    "sales = pd.read_excel('SalesCS.xlsx')\n",
    "# Examine the sales dataset to ensure it has been read in accurately and examine the data contained\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>AccountDesc</th>\n",
       "      <th>TransDesc</th>\n",
       "      <th>Debit</th>\n",
       "      <th>Credit</th>\n",
       "      <th>Period</th>\n",
       "      <th>JnlNo</th>\n",
       "      <th>JnlDesc</th>\n",
       "      <th>Amount</th>\n",
       "      <th>JnlPrep</th>\n",
       "      <th>JnlAuth</th>\n",
       "      <th>JnlDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-80-8033</td>\n",
       "      <td>Provision for Sales Schemes</td>\n",
       "      <td>ZZX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>-9668.59</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-10-1002</td>\n",
       "      <td>Provisions - Trade Sales</td>\n",
       "      <td>XXX</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-80-8033</td>\n",
       "      <td>Provision for Sales Schemes</td>\n",
       "      <td>ZZX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>-291191.30</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-10-1001</td>\n",
       "      <td>Trade Sale Recycle Scheme</td>\n",
       "      <td>ZZZ</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-20-2004</td>\n",
       "      <td>Provision for Obselete Inventory</td>\n",
       "      <td>923</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12848.50</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Reversed By Jnl 2019-4 Journal No. 366</td>\n",
       "      <td>-12848.50</td>\n",
       "      <td>DF18</td>\n",
       "      <td>TC01</td>\n",
       "      <td>30/01/2019 09:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account                       AccountDesc TransDesc      Debit  \\\n",
       "0  00-80-8033       Provision for Sales Schemes       ZZX       0.00   \n",
       "1  00-10-1002          Provisions - Trade Sales       XXX    9668.59   \n",
       "2  00-80-8033       Provision for Sales Schemes       ZZX       0.00   \n",
       "3  00-10-1001         Trade Sale Recycle Scheme       ZZZ  291191.30   \n",
       "4  00-20-2004  Provision for Obselete Inventory       923       0.00   \n",
       "\n",
       "      Credit  Period  JnlNo                                 JnlDesc  \\\n",
       "0    9668.59  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "1       0.00  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "2  291191.30  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "3       0.00  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "4   12848.50  2019-1     10  Reversed By Jnl 2019-4 Journal No. 366   \n",
       "\n",
       "      Amount JnlPrep JnlAuth       JnlDateTime  \n",
       "0   -9668.59    HV09    AS13  01/01/2019 13:04  \n",
       "1    9668.59    HV09    AS13  01/01/2019 13:04  \n",
       "2 -291191.30    HV09    AS13  01/01/2019 13:04  \n",
       "3  291191.30    HV09    AS13  01/01/2019 13:04  \n",
       "4  -12848.50    DF18    TC01  30/01/2019 09:56  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the first journals dataset as journals1\n",
    "journals1= pd.read_csv('Journals Part 1.csv')\n",
    "\n",
    "# Examine it to ensure it has been read in accurately and examine the data contained\n",
    "journals1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>AccountDesc</th>\n",
       "      <th>TransDesc</th>\n",
       "      <th>Debit</th>\n",
       "      <th>Credit</th>\n",
       "      <th>Period</th>\n",
       "      <th>JnlNo</th>\n",
       "      <th>JnlDesc</th>\n",
       "      <th>Amount</th>\n",
       "      <th>JnlPrep</th>\n",
       "      <th>JnlAuth</th>\n",
       "      <th>JnlDateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-80-8043</td>\n",
       "      <td>Tax Control</td>\n",
       "      <td>XXX</td>\n",
       "      <td>19763.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-7</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19763.35</td>\n",
       "      <td>DF18</td>\n",
       "      <td>AM04</td>\n",
       "      <td>01/07/2019 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-80-8044</td>\n",
       "      <td>Tax Input</td>\n",
       "      <td>XZZX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19763.35</td>\n",
       "      <td>2019-7</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19763.35</td>\n",
       "      <td>DF18</td>\n",
       "      <td>AM04</td>\n",
       "      <td>01/07/2019 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-80-8043</td>\n",
       "      <td>Tax Control</td>\n",
       "      <td>XXX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22956.90</td>\n",
       "      <td>2019-7</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22956.90</td>\n",
       "      <td>DF18</td>\n",
       "      <td>AM04</td>\n",
       "      <td>01/07/2019 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-80-8045</td>\n",
       "      <td>Tax Output</td>\n",
       "      <td>ZZXXZ</td>\n",
       "      <td>22956.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-7</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22956.90</td>\n",
       "      <td>DF18</td>\n",
       "      <td>AM04</td>\n",
       "      <td>01/07/2019 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-80-8045</td>\n",
       "      <td>Tax Output</td>\n",
       "      <td>ZZXXZ</td>\n",
       "      <td>40240.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-7</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40240.30</td>\n",
       "      <td>DF18</td>\n",
       "      <td>AM04</td>\n",
       "      <td>01/07/2019 16:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account  AccountDesc TransDesc     Debit    Credit  Period  JnlNo  \\\n",
       "0  00-80-8043  Tax Control       XXX  19763.35      0.00  2019-7    253   \n",
       "1  00-80-8044    Tax Input      XZZX      0.00  19763.35  2019-7    253   \n",
       "2  00-80-8043  Tax Control       XXX      0.00  22956.90  2019-7    253   \n",
       "3  00-80-8045   Tax Output     ZZXXZ  22956.90      0.00  2019-7    253   \n",
       "4  00-80-8045   Tax Output     ZZXXZ  40240.30      0.00  2019-7    253   \n",
       "\n",
       "  JnlDesc    Amount JnlPrep JnlAuth       JnlDateTime  \n",
       "0     NaN  19763.35    DF18    AM04  01/07/2019 16:44  \n",
       "1     NaN -19763.35    DF18    AM04  01/07/2019 16:44  \n",
       "2     NaN -22956.90    DF18    AM04  01/07/2019 16:44  \n",
       "3     NaN  22956.90    DF18    AM04  01/07/2019 16:44  \n",
       "4     NaN  40240.30    DF18    AM04  01/07/2019 16:44  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the second journals dataset as journals2\n",
    "journals2 = pd.read_csv('Journals Part 2.csv')\n",
    "\n",
    "# Examine it to ensure it has been read in accurately and examine the data contained\n",
    "journals2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining and setting data types\n",
    "\n",
    "As we can see from our first glance at the data, the dataset has a mixture of numerical and non-numerical features. By examining the data types, we can see how Python has interpreted these features and we can change the types of any fields, as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51308 entries, 0 to 51307\n",
      "Data columns (total 24 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Row ID          51308 non-null  int64         \n",
      " 1   Order ID        51308 non-null  object        \n",
      " 2   Order Date      51308 non-null  datetime64[ns]\n",
      " 3   Ship Date       51308 non-null  datetime64[ns]\n",
      " 4   Ship Mode       51308 non-null  object        \n",
      " 5   Customer ID     51308 non-null  object        \n",
      " 6   Customer Name   51308 non-null  object        \n",
      " 7   Segment         51286 non-null  object        \n",
      " 8   Postal Code     9998 non-null   float64       \n",
      " 9   City            51308 non-null  object        \n",
      " 10  State           51308 non-null  object        \n",
      " 11  Country         51308 non-null  object        \n",
      " 12  Region          51308 non-null  object        \n",
      " 13  Market          51248 non-null  object        \n",
      " 14  Product ID      51308 non-null  object        \n",
      " 15  Category        51308 non-null  object        \n",
      " 16  Sub-Category    51308 non-null  object        \n",
      " 17  Product Name    51308 non-null  object        \n",
      " 18  Sales           51308 non-null  float64       \n",
      " 19  Quantity        51308 non-null  int64         \n",
      " 20  Discount        51308 non-null  float64       \n",
      " 21  Profit          51308 non-null  float64       \n",
      " 22  Shipping Cost   51308 non-null  float64       \n",
      " 23  Order Priority  51308 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(5), int64(2), object(15)\n",
      "memory usage: 9.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Examine the data types of the sales data\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Account      1800 non-null   object \n",
      " 1   AccountDesc  1800 non-null   object \n",
      " 2   TransDesc    1800 non-null   object \n",
      " 3   Debit        1800 non-null   float64\n",
      " 4   Credit       1800 non-null   float64\n",
      " 5   Period       1800 non-null   object \n",
      " 6   JnlNo        1800 non-null   int64  \n",
      " 7   JnlDesc      316 non-null    object \n",
      " 8   Amount       1800 non-null   float64\n",
      " 9   JnlPrep      1800 non-null   object \n",
      " 10  JnlAuth      1800 non-null   object \n",
      " 11  JnlDateTime  1783 non-null   object \n",
      "dtypes: float64(3), int64(1), object(8)\n",
      "memory usage: 168.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Examine the data types of the first journals data\n",
    "journals1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2201 entries, 0 to 2200\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Account      2201 non-null   object \n",
      " 1   AccountDesc  2201 non-null   object \n",
      " 2   TransDesc    2201 non-null   object \n",
      " 3   Debit        2201 non-null   float64\n",
      " 4   Credit       2201 non-null   float64\n",
      " 5   Period       2201 non-null   object \n",
      " 6   JnlNo        2201 non-null   int64  \n",
      " 7   JnlDesc      400 non-null    object \n",
      " 8   Amount       2201 non-null   float64\n",
      " 9   JnlPrep      2201 non-null   object \n",
      " 10  JnlAuth      2201 non-null   object \n",
      " 11  JnlDateTime  2201 non-null   object \n",
      "dtypes: float64(3), int64(1), object(8)\n",
      "memory usage: 206.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Examine the data types of the second journals data\n",
    "journals2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set any data types, if required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying any data quality issues\n",
    "\n",
    "We've now examined the data fields and types contained in our data. Next, we should examine the quality of our data. Two of the biggest data quality issues that can affect and distort our analysis are duplicates and missing data. \n",
    "\n",
    "Now, examine the datasets provided to identify if there are any duplicate rows or entries in the dataset and if any fields have missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered how to check for duplicates and for missing values within a dataset in Module 3 of Unit 2: Data Wrangling ‘Duplicates and missing datasets’. If you purchased the learning and certificate Analyst Pathway, we strongly recommend you revisit this content if you are struggling to complete these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of duplicates in the journal dataset\n",
    "journals = pd.concat([journals1,journals2])\n",
    "duplicates = journals.duplicated()\n",
    "duplicated_values = journals[duplicates].sort_values(by = 'Amount')\n",
    "print(duplicated_values['JnlNo'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the number of duplicate journal entries.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is assessed in the assessment.\n",
    "\n",
    "We can examine the duplicates by using the duplicated() command. This produces a boolean indicating whether each row is a duplicate or not, with True signifying the sales order is a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of duplicates in the sales dataset\n",
    "duplicates = sales.duplicated()\n",
    "duplicated_values = sales[duplicates].sort_values(by = 'Order ID')\n",
    "print(duplicated_values['Order ID'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the number of duplicate sales orders.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is assessed in the assessment.\n",
    "\n",
    "We can identify missing values by using the is.na() command, which returns a boolean for each data point on whether it is a missing value or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Account           0\n",
       "AccountDesc       0\n",
       "TransDesc         0\n",
       "Debit             0\n",
       "Credit            0\n",
       "Period            0\n",
       "JnlNo             0\n",
       "JnlDesc        3285\n",
       "Amount            0\n",
       "JnlPrep           0\n",
       "JnlAuth           0\n",
       "JnlDateTime      17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine if there are any missing values in the first journal dataset\n",
    "journals.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the fields which have missing values in the journals dataset.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: JnlDesc, JnlDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row ID                0\n",
       "Order ID              0\n",
       "Order Date            0\n",
       "Ship Date             0\n",
       "Ship Mode             0\n",
       "Customer ID           0\n",
       "Customer Name         0\n",
       "Segment              22\n",
       "Postal Code       41310\n",
       "City                  0\n",
       "State                 0\n",
       "Country               0\n",
       "Region                0\n",
       "Market               60\n",
       "Product ID            0\n",
       "Category              0\n",
       "Sub-Category          0\n",
       "Product Name          0\n",
       "Sales                 0\n",
       "Quantity              0\n",
       "Discount              0\n",
       "Profit                0\n",
       "Shipping Cost         0\n",
       "Order Priority        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine if there are any missing values in the sales dataset\n",
    "sales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is assessed in the assessment.\n",
    "\n",
    "We can identify missing values by using the is.na() command, which returns a boolean for each data point on whether it is a missing value or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the number of missing values in the Segment column of the sales dataset.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling and resolving data quality issues\n",
    "\n",
    "We have identified that there are numerous missing values or duplicate entries in all of the datasets. First we will handle the missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "There are several fields in the data that have missing values to a different extent. The best way to handle these fields depends on what is missing. \n",
    "\n",
    "First, let's consider the sales data. We have identified there are missing values for the customer's post code, the order's market and the customer segment. Due to the amount of missing values for Post Code and the fact it is unlikely we could infer them, it may be best to drop Post Code from our sales dataset. However, it may be possible to suggest a suitable value for customer segment and order's market.\n",
    "\n",
    "While we examined missing values in Module 3 of Unit 2: Data Wrangling, to identify these values we will use the practical skills of subsetting and selecting data that we learnt in Module 2 of Unit 3: Analysing the Data. If you purchased the learning and certificate Analyst Pathway, we recommend you revisit this content  if you are struggling to complete these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Post Code from the sales data\n",
    "sales = sales.drop('Postal Code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "51303    False\n",
       "51304    False\n",
       "51305    False\n",
       "51306    False\n",
       "51307    False\n",
       "Name: Market, Length: 51308, dtype: bool"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the rows which have missing values for Market in the sales data and examine the regions\n",
    "sales['Market'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>...</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Shipping Cost</th>\n",
       "      <th>Order Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>4260</td>\n",
       "      <td>MX-2013-AS1004598-41501</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>AS-1004598</td>\n",
       "      <td>Aaron Smayling</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Panama City</td>\n",
       "      <td>Panama</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-SU-4117</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>Elite Box Cutter, High Speed</td>\n",
       "      <td>13.956</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-5.5840</td>\n",
       "      <td>1.252</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>7313</td>\n",
       "      <td>US-2012-AB1025582-40928</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>AB-1025582</td>\n",
       "      <td>Alejandro Ballentine</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>Distrito Federal</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-BO-3640</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Library with Doors, Mobile</td>\n",
       "      <td>195.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-34.2520</td>\n",
       "      <td>17.083</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>347</td>\n",
       "      <td>MX-2015-AG1030082-42367</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>First Class</td>\n",
       "      <td>AG-1030082</td>\n",
       "      <td>Aleksandra Gannaway</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Morelia</td>\n",
       "      <td>Michoacán</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-EN-5030</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Envelopes</td>\n",
       "      <td>Kraft Interoffice Envelope, Security-Tint</td>\n",
       "      <td>328.800</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.8000</td>\n",
       "      <td>58.451</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>912</td>\n",
       "      <td>MX-2012-BP1109582-40933</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>BP-1109582</td>\n",
       "      <td>Bart Pistole</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Coatzacoalcos</td>\n",
       "      <td>Veracruz</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-EN-4452</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Envelopes</td>\n",
       "      <td>GlobeWeis Peel and Seal, Security-Tint</td>\n",
       "      <td>31.720</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1200</td>\n",
       "      <td>4.451</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>885</td>\n",
       "      <td>MX-2013-BF1121551-41503</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>BF-1121551</td>\n",
       "      <td>Benjamin Farhat</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Huehuetenango</td>\n",
       "      <td>Huehuetenango</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-TA-3356</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Barricks Wood Table, Adjustable Height</td>\n",
       "      <td>276.864</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>96.8840</td>\n",
       "      <td>31.932</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50935</th>\n",
       "      <td>38348</td>\n",
       "      <td>CA-2014-WB21850140-41985</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>WB-218501404</td>\n",
       "      <td>William Brown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>California</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-AR-3518</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "      <td>Boston 16765 Mini Stand Up Battery Pencil Shar...</td>\n",
       "      <td>23.320</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0632</td>\n",
       "      <td>6.790</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50936</th>\n",
       "      <td>38357</td>\n",
       "      <td>CA-2014-WB21850140-41985</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>WB-218501404</td>\n",
       "      <td>William Brown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>California</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-BI-3506</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Black Avery Memo-Size 3-Ring Binder, 5 1/2\" x ...</td>\n",
       "      <td>5.872</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1286</td>\n",
       "      <td>1.650</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51296</th>\n",
       "      <td>26172</td>\n",
       "      <td>IN-2012-ZD2192511-41247</td>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>ZD-2192511</td>\n",
       "      <td>Zuschuss Donatelli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-AR-3483</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "      <td>Binney &amp; Smith Highlighters, Fluorescent</td>\n",
       "      <td>107.100</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8600</td>\n",
       "      <td>14.800</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51297</th>\n",
       "      <td>26173</td>\n",
       "      <td>IN-2012-ZD2192511-41247</td>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>ZD-2192511</td>\n",
       "      <td>Zuschuss Donatelli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-BI-6385</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Wilson Jones Binding Machine, Recycled</td>\n",
       "      <td>98.640</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.5600</td>\n",
       "      <td>12.100</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51305</th>\n",
       "      <td>18671</td>\n",
       "      <td>ES-2012-ZD2192564-40960</td>\n",
       "      <td>2016-02-21</td>\n",
       "      <td>2016-02-26</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>ZD-2192564</td>\n",
       "      <td>Zuschuss Donatelli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Milan</td>\n",
       "      <td>Lombardy</td>\n",
       "      <td>...</td>\n",
       "      <td>OFF-AR-3478</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "      <td>Binney &amp; Smith Canvas, Water Color</td>\n",
       "      <td>163.620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.4800</td>\n",
       "      <td>16.050</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Row ID                  Order ID Order Date  Ship Date       Ship Mode  \\\n",
       "190      4260   MX-2013-AS1004598-41501 2017-08-15 2017-08-20  Standard Class   \n",
       "1127     7313   US-2012-AB1025582-40928 2016-01-20 2016-01-25  Standard Class   \n",
       "1272      347   MX-2015-AG1030082-42367 2019-12-29 2020-01-01     First Class   \n",
       "4664      912   MX-2012-BP1109582-40933 2016-01-25 2016-01-29  Standard Class   \n",
       "5260      885   MX-2013-BF1121551-41503 2017-08-17 2017-08-19    Second Class   \n",
       "...       ...                       ...        ...        ...             ...   \n",
       "50935   38348  CA-2014-WB21850140-41985 2018-12-12 2018-12-12        Same Day   \n",
       "50936   38357  CA-2014-WB21850140-41985 2018-12-12 2018-12-12        Same Day   \n",
       "51296   26172   IN-2012-ZD2192511-41247 2016-12-04 2016-12-09  Standard Class   \n",
       "51297   26173   IN-2012-ZD2192511-41247 2016-12-04 2016-12-09  Standard Class   \n",
       "51305   18671   ES-2012-ZD2192564-40960 2016-02-21 2016-02-26  Standard Class   \n",
       "\n",
       "        Customer ID         Customer Name      Segment           City  \\\n",
       "190      AS-1004598        Aaron Smayling    Corporate    Panama City   \n",
       "1127     AB-1025582  Alejandro Ballentine  Home Office    Mexico City   \n",
       "1272     AG-1030082   Aleksandra Gannaway    Corporate        Morelia   \n",
       "4664     BP-1109582          Bart Pistole    Corporate  Coatzacoalcos   \n",
       "5260     BF-1121551       Benjamin Farhat  Home Office  Huehuetenango   \n",
       "...             ...                   ...          ...            ...   \n",
       "50935  WB-218501404         William Brown          NaN        Anaheim   \n",
       "50936  WB-218501404         William Brown          NaN        Anaheim   \n",
       "51296    ZD-2192511    Zuschuss Donatelli          NaN          Dhaka   \n",
       "51297    ZD-2192511    Zuschuss Donatelli          NaN          Dhaka   \n",
       "51305    ZD-2192564    Zuschuss Donatelli          NaN          Milan   \n",
       "\n",
       "                  State  ...   Product ID         Category Sub-Category  \\\n",
       "190              Panama  ...  OFF-SU-4117  Office Supplies     Supplies   \n",
       "1127   Distrito Federal  ...  FUR-BO-3640        Furniture    Bookcases   \n",
       "1272          Michoacán  ...  OFF-EN-5030  Office Supplies    Envelopes   \n",
       "4664           Veracruz  ...  OFF-EN-4452  Office Supplies    Envelopes   \n",
       "5260      Huehuetenango  ...  FUR-TA-3356        Furniture       Tables   \n",
       "...                 ...  ...          ...              ...          ...   \n",
       "50935        California  ...  OFF-AR-3518  Office Supplies          Art   \n",
       "50936        California  ...  OFF-BI-3506  Office Supplies      Binders   \n",
       "51296             Dhaka  ...  OFF-AR-3483  Office Supplies          Art   \n",
       "51297             Dhaka  ...  OFF-BI-6385  Office Supplies      Binders   \n",
       "51305          Lombardy  ...  OFF-AR-3478  Office Supplies          Art   \n",
       "\n",
       "                                            Product Name    Sales Quantity  \\\n",
       "190                         Elite Box Cutter, High Speed   13.956        1   \n",
       "1127                     Bush Library with Doors, Mobile  195.648        1   \n",
       "1272           Kraft Interoffice Envelope, Security-Tint  328.800       10   \n",
       "4664              GlobeWeis Peel and Seal, Security-Tint   31.720        2   \n",
       "5260              Barricks Wood Table, Adjustable Height  276.864        1   \n",
       "...                                                  ...      ...      ...   \n",
       "50935  Boston 16765 Mini Stand Up Battery Pencil Shar...   23.320        2   \n",
       "50936  Black Avery Memo-Size 3-Ring Binder, 5 1/2\" x ...    5.872        2   \n",
       "51296           Binney & Smith Highlighters, Fluorescent  107.100        6   \n",
       "51297             Wilson Jones Binding Machine, Recycled   98.640        2   \n",
       "51305                 Binney & Smith Canvas, Water Color  163.620        3   \n",
       "\n",
       "      Discount    Profit  Shipping Cost  Order Priority  \n",
       "190        0.4   -5.5840          1.252          Medium  \n",
       "1127       0.2  -34.2520         17.083          Medium  \n",
       "1272       0.0  147.8000         58.451          Medium  \n",
       "4664       0.0   10.1200          4.451            High  \n",
       "5260       0.2   96.8840         31.932          Medium  \n",
       "...        ...       ...            ...             ...  \n",
       "50935      0.0    6.0632          6.790        Critical  \n",
       "50936      0.2    2.1286          1.650        Critical  \n",
       "51296      0.0   13.8600         14.800            High  \n",
       "51297      0.0   31.5600         12.100            High  \n",
       "51305      0.0   78.4800         16.050          Medium  \n",
       "\n",
       "[82 rows x 23 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the missing value for Market in the sales data using the region field\n",
    "sales[sales.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values for Market in the sales data and save the changes to this field\n",
    "sales['Market'] = sales['Market'].fillna('Unkown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17056           Erica Smith\n",
       "17057           Erica Smith\n",
       "17058           Erica Smith\n",
       "17059           Erica Smith\n",
       "17060           Erica Smith\n",
       "46902          Susan Pistek\n",
       "46903          Susan Pistek\n",
       "46904          Susan Pistek\n",
       "46938          Susan Pistek\n",
       "46939          Susan Pistek\n",
       "46940          Susan Pistek\n",
       "50929         William Brown\n",
       "50930         William Brown\n",
       "50931         William Brown\n",
       "50932         William Brown\n",
       "50933         William Brown\n",
       "50934         William Brown\n",
       "50935         William Brown\n",
       "50936         William Brown\n",
       "51296    Zuschuss Donatelli\n",
       "51297    Zuschuss Donatelli\n",
       "51305    Zuschuss Donatelli\n",
       "Name: Customer Name, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the rows that have missing values for Customer Segment in the sales data and create a subset or list of the Customer Names\n",
    "missing = sales[sales.isnull().any(axis=1)]\n",
    "missingsubset = missing['Customer Name']\n",
    "missingsubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Row ID                  Order ID Order Date  Ship Date       Ship Mode  \\\n",
      "17037   10625   ES-2015-ES1402048-42369 2019-12-31 2020-01-04  Standard Class   \n",
      "17038   12057   ES-2015-ES1402045-42343 2019-12-05 2019-12-10  Standard Class   \n",
      "17039   12058   ES-2015-ES1402045-42343 2019-12-05 2019-12-10  Standard Class   \n",
      "17040   12056   ES-2015-ES1402045-42343 2019-12-05 2019-12-10  Standard Class   \n",
      "17041   45974    KE-2015-ES402069-42341 2019-12-03 2019-12-03        Same Day   \n",
      "...       ...                       ...        ...        ...             ...   \n",
      "17099   49918    BO-2012-ES402013-41147 2016-08-26 2016-09-01  Standard Class   \n",
      "17100    1647   US-2012-ES1402018-41107 2016-07-17 2016-07-21  Standard Class   \n",
      "17101   17296   ES-2012-ES1402045-41102 2016-07-12 2016-07-14     First Class   \n",
      "17102   26010  IN-2012-ES14020130-41047 2016-05-18 2016-05-23    Second Class   \n",
      "17103   26009  IN-2012-ES14020130-41047 2016-05-18 2016-05-23    Second Class   \n",
      "\n",
      "       Customer ID Customer Name   Segment                   City  \\\n",
      "17037   ES-1402048   Erica Smith  Consumer              Bielefeld   \n",
      "17038   ES-1402045   Erica Smith  Consumer  Pierrefitte-sur-Seine   \n",
      "17039   ES-1402045   Erica Smith  Consumer  Pierrefitte-sur-Seine   \n",
      "17040   ES-1402045   Erica Smith  Consumer  Pierrefitte-sur-Seine   \n",
      "17041    ES-402069   Erica Smith  Consumer                 Kisumu   \n",
      "...            ...           ...       ...                    ...   \n",
      "17099    ES-402013   Erica Smith  Consumer                  Brest   \n",
      "17100   ES-1402018   Erica Smith  Consumer               Salvador   \n",
      "17101   ES-1402045   Erica Smith  Consumer               Louviers   \n",
      "17102  ES-14020130   Erica Smith  Consumer                Bangkok   \n",
      "17103  ES-14020130   Erica Smith  Consumer                Bangkok   \n",
      "\n",
      "                        State  ...   Product ID         Category Sub-Category  \\\n",
      "17037  North Rhine-Westphalia  ...  OFF-SU-4991  Office Supplies     Supplies   \n",
      "17038           Ile-de-France  ...  OFF-PA-3998  Office Supplies        Paper   \n",
      "17039           Ile-de-France  ...  OFF-ST-4258  Office Supplies      Storage   \n",
      "17040           Ile-de-France  ...  OFF-EN-4434  Office Supplies    Envelopes   \n",
      "17041                  Nyanza  ...  FUR-BO-4853        Furniture    Bookcases   \n",
      "...                       ...  ...          ...              ...          ...   \n",
      "17099                   Brest  ...  OFF-BI-2918  Office Supplies      Binders   \n",
      "17100                   Bahia  ...  OFF-AR-6120  Office Supplies          Art   \n",
      "17101                Normandy  ...  OFF-ST-4061  Office Supplies      Storage   \n",
      "17102                 Bangkok  ...  OFF-SU-4986  Office Supplies     Supplies   \n",
      "17103                 Bangkok  ...  OFF-EN-5036  Office Supplies    Envelopes   \n",
      "\n",
      "                                            Product Name     Sales Quantity  \\\n",
      "17037                          Kleencut Shears, Serrated  260.8200        6   \n",
      "17038                      Eaton Message Books, 8.5 x 11  123.6000        5   \n",
      "17039                     Fellowes File Cart, Industrial  248.0220        2   \n",
      "17040  GlobeWeis Business Envelopes, with clear poly ...   41.7600        2   \n",
      "17041                       Ikea Corner Shelving, Mobile  251.1600        2   \n",
      "...                                                  ...       ...      ...   \n",
      "17099                            Acco Index Tab, Durable   17.8800        2   \n",
      "17100              Stanley Pencil Sharpener, Water Color   44.7360        6   \n",
      "17101                          Eldon Folders, Industrial   78.5700        5   \n",
      "17102                      Kleencut Scissors, High Speed   35.4411        3   \n",
      "17103              Kraft Mailers, with clear poly window   65.4921        3   \n",
      "\n",
      "      Discount   Profit  Shipping Cost  Order Priority  \n",
      "17037     0.00  12.9600         15.790            High  \n",
      "17038     0.00  54.3000          8.260          Medium  \n",
      "17039     0.10   8.2620          8.020          Medium  \n",
      "17040     0.00  12.4800          2.760          Medium  \n",
      "17041     0.00  32.6400         41.850        Critical  \n",
      "...        ...      ...            ...             ...  \n",
      "17099     0.00   3.9000          1.110          Medium  \n",
      "17100     0.60 -45.8640          2.657          Medium  \n",
      "17101     0.10  -6.1800         25.560            High  \n",
      "17102     0.47  -2.7189          4.330          Medium  \n",
      "17103     0.47   1.2321          3.050          Medium  \n",
      "\n",
      "[67 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Examine the sales orders for all customers in the list and identify the missing value for Segment\n",
    "print(sales[sales['Customer Name'] == 'Erica Smith'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/gwqqg3ms3hz63rft5zbg43700000gn/T/ipykernel_36006/2408762158.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  sales.fillna(method = 'ffill', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Resolve the missing values for Customer Segment in the sales data and save the changes to this field\n",
    "sales.sort_values('Customer Name')\n",
    "sales.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified that several journal entries have missing values for the time and date the journal was posted. We may be able to infer these values from the rest of the journals data.\n",
    "\n",
    "The following is assessed in the assessment.\n",
    "\n",
    "We can identify the missing journal date and time by examining the other journal entries from the same journal number. This is because these will have been posted together at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027    NaN\n",
       "1028    NaN\n",
       "1029    NaN\n",
       "1030    NaN\n",
       "1031    NaN\n",
       "1032    NaN\n",
       "1033    NaN\n",
       "1034    NaN\n",
       "1035    NaN\n",
       "1036    NaN\n",
       "1037    NaN\n",
       "1038    NaN\n",
       "1039    NaN\n",
       "1040    NaN\n",
       "1041    NaN\n",
       "1042    NaN\n",
       "1043    NaN\n",
       "Name: JnlDateTime, dtype: object"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the rows of the missing values for Journal Date Time\n",
    "journals.JnlDateTime[journals.JnlDateTime.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020    30/06/2019 09:26\n",
       "1021    30/06/2019 09:26\n",
       "1022    30/06/2019 09:26\n",
       "1023    30/06/2019 09:26\n",
       "1024    31/01/2019 12:27\n",
       "1025    31/01/2019 12:27\n",
       "1026    31/01/2019 12:27\n",
       "1027                 NaN\n",
       "1028                 NaN\n",
       "1029                 NaN\n",
       "1030                 NaN\n",
       "1031                 NaN\n",
       "1032                 NaN\n",
       "1033                 NaN\n",
       "1034                 NaN\n",
       "1035                 NaN\n",
       "1036                 NaN\n",
       "1037                 NaN\n",
       "1038                 NaN\n",
       "1039                 NaN\n",
       "1040                 NaN\n",
       "1041                 NaN\n",
       "1042                 NaN\n",
       "1043                 NaN\n",
       "1044    30/06/2019 12:47\n",
       "1045    30/06/2019 12:47\n",
       "1046    30/06/2019 15:15\n",
       "1047    30/06/2019 15:15\n",
       "1048    30/06/2019 15:15\n",
       "1049    30/06/2019 15:15\n",
       "Name: JnlDateTime, dtype: object"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the missing values for Journal Date Time\n",
    "journals.sort_values('JnlDateTime')\n",
    "journals['JnlDateTime'][1020:1050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the missing value in Journal Date Time.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: 31/01/2019 12:27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/gwqqg3ms3hz63rft5zbg43700000gn/T/ipykernel_36006/2940783724.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  journals.fillna(method = 'ffill', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Fill in the missing values for Journal Date Time \n",
    "journals['JnlDesc'].fillna('Blank')\n",
    "journals.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020    30/06/2019 09:26\n",
       "1021    30/06/2019 09:26\n",
       "1022    30/06/2019 09:26\n",
       "1023    30/06/2019 09:26\n",
       "1024    31/01/2019 12:27\n",
       "1025    31/01/2019 12:27\n",
       "1026    31/01/2019 12:27\n",
       "1027    31/01/2019 12:27\n",
       "1028    31/01/2019 12:27\n",
       "1029    31/01/2019 12:27\n",
       "1030    31/01/2019 12:27\n",
       "1031    31/01/2019 12:27\n",
       "1032    31/01/2019 12:27\n",
       "1033    31/01/2019 12:27\n",
       "1034    31/01/2019 12:27\n",
       "1035    31/01/2019 12:27\n",
       "1036    31/01/2019 12:27\n",
       "1037    31/01/2019 12:27\n",
       "1038    31/01/2019 12:27\n",
       "1039    31/01/2019 12:27\n",
       "1040    31/01/2019 12:27\n",
       "1041    31/01/2019 12:27\n",
       "1042    31/01/2019 12:27\n",
       "1043    31/01/2019 12:27\n",
       "1044    30/06/2019 12:47\n",
       "1045    30/06/2019 12:47\n",
       "1046    30/06/2019 15:15\n",
       "1047    30/06/2019 15:15\n",
       "1048    30/06/2019 15:15\n",
       "1049    30/06/2019 15:15\n",
       "Name: JnlDateTime, dtype: object"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the missing values have been filled in\n",
    "journals['JnlDateTime'][1020:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "We have identified that there are duplicate entries in all of the datasets.\n",
    "\n",
    "In the first journals dataset, there is a whole journal that has been duplicated and a few duplicated journal entries. This is also the case for the second journals dataset. Lastly, there are several sales orders that are duplicated in the sales data. To avoid these compromising our analysis, we will remove these duplicates.\n",
    "\n",
    "We have examined how to drop duplicates in Module 3 of Unit 2: Data Wrangling. If you purchased the learning and certificate Analyst Pathway, we recommend you revisit this content if you are struggling to complete these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicates in the journals data and save this dataset as journals\n",
    "journals.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicates in the sales data and save this dataset as sales\n",
    "sales.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling the data\n",
    "\n",
    "Now we have resolved our data quality issues, we can wrangle our data into a usable format for our analysis. \n",
    "\n",
    "Until now, we have been using three datasets, two of which are journal datasets covering different periods. For our analysis, it is better to use a single journal dataset to allow us to examine and analyse the journals data as a whole.\n",
    "\n",
    "We practised appending the journals data and performing joins in Module 2 of Unit 2: Data Wrangling. If you purchased the learning and certificate Analyst Pathway, we recommend you revisit this content if you are struggling to complete these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In addition to the journal datasets provided, we have obtained a dataset containing the employee names for the accounts team responsbile for preparing and authorising the journals. It will be beneficial to include this information in the journal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeRef</th>\n",
       "      <th>Employee Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC04</td>\n",
       "      <td>Lorelei Ory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AH12</td>\n",
       "      <td>Latasha Terpstra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JC39</td>\n",
       "      <td>Merrill Benzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID03</td>\n",
       "      <td>Evalyn Reddout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV09</td>\n",
       "      <td>Lakeisha Testerman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmployeeRef         Employee Name\n",
       "0        AC04         Lorelei Ory  \n",
       "1        AH12    Latasha Terpstra  \n",
       "2        JC39      Merrill Benzel  \n",
       "3        ID03      Evalyn Reddout  \n",
       "4        HV09  Lakeisha Testerman  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Accounts Team dataset as AccountsTeam\n",
    "accountsteam = pd.read_csv('Accounts Team Staff.csv')\n",
    "# Examine the data\n",
    "accountsteam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Accounts Team data to the journals dataset\n",
    "\n",
    "# First, join on the Journal Preparer\n",
    "journals = journals.merge(accountsteam, how = 'left', left_on = 'JnlPrep', right_on = 'EmployeeRef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the new column as JnlPreparerName and drop redundant columns EmployeeRef and Employee Name \n",
    "journals['JnlPreparerName'] = journals['Employee Name']\n",
    "journals = journals.drop(columns = ['EmployeeRef','Employee Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, join on the Journal Authoriser field\n",
    "journals = journals.merge(accountsteam, how = 'left', left_on='JnlAuth',right_on = 'EmployeeRef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again rename new column as JnlAuthoriserName and drop redundant columns EmployeeRef and Employee Name \n",
    "journals['JnlAuthoriserName'] = journals['Employee Name']\n",
    "journals = journals.drop(columns = ['EmployeeRef', 'Employee Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>AccountDesc</th>\n",
       "      <th>TransDesc</th>\n",
       "      <th>Debit</th>\n",
       "      <th>Credit</th>\n",
       "      <th>Period</th>\n",
       "      <th>JnlNo</th>\n",
       "      <th>JnlDesc</th>\n",
       "      <th>Amount</th>\n",
       "      <th>JnlPrep</th>\n",
       "      <th>JnlAuth</th>\n",
       "      <th>JnlDateTime</th>\n",
       "      <th>JnlPreparerName</th>\n",
       "      <th>JnlAuthoriserName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-80-8033</td>\n",
       "      <td>Provision for Sales Schemes</td>\n",
       "      <td>ZZX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>-9668.59</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "      <td>Lakeisha Testerman</td>\n",
       "      <td>Jonelle Moseley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-10-1002</td>\n",
       "      <td>Provisions - Trade Sales</td>\n",
       "      <td>XXX</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>9668.59</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "      <td>Lakeisha Testerman</td>\n",
       "      <td>Jonelle Moseley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-80-8033</td>\n",
       "      <td>Provision for Sales Schemes</td>\n",
       "      <td>ZZX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>-291191.30</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "      <td>Lakeisha Testerman</td>\n",
       "      <td>Jonelle Moseley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-10-1001</td>\n",
       "      <td>Trade Sale Recycle Scheme</td>\n",
       "      <td>ZZZ</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reversed By Jnl 2019-9 Journal No. 277</td>\n",
       "      <td>291191.30</td>\n",
       "      <td>HV09</td>\n",
       "      <td>AS13</td>\n",
       "      <td>01/01/2019 13:04</td>\n",
       "      <td>Lakeisha Testerman</td>\n",
       "      <td>Jonelle Moseley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-20-2004</td>\n",
       "      <td>Provision for Obselete Inventory</td>\n",
       "      <td>923</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12848.50</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>10</td>\n",
       "      <td>Reversed By Jnl 2019-4 Journal No. 366</td>\n",
       "      <td>-12848.50</td>\n",
       "      <td>DF18</td>\n",
       "      <td>TC01</td>\n",
       "      <td>30/01/2019 09:56</td>\n",
       "      <td>Jon Mckinley</td>\n",
       "      <td>Johnny Hevey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account                       AccountDesc TransDesc      Debit  \\\n",
       "0  00-80-8033       Provision for Sales Schemes       ZZX       0.00   \n",
       "1  00-10-1002          Provisions - Trade Sales       XXX    9668.59   \n",
       "2  00-80-8033       Provision for Sales Schemes       ZZX       0.00   \n",
       "3  00-10-1001         Trade Sale Recycle Scheme       ZZZ  291191.30   \n",
       "4  00-20-2004  Provision for Obselete Inventory       923       0.00   \n",
       "\n",
       "      Credit  Period  JnlNo                                 JnlDesc  \\\n",
       "0    9668.59  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "1       0.00  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "2  291191.30  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "3       0.00  2019-1      1  Reversed By Jnl 2019-9 Journal No. 277   \n",
       "4   12848.50  2019-1     10  Reversed By Jnl 2019-4 Journal No. 366   \n",
       "\n",
       "      Amount JnlPrep JnlAuth       JnlDateTime       JnlPreparerName  \\\n",
       "0   -9668.59    HV09    AS13  01/01/2019 13:04  Lakeisha Testerman     \n",
       "1    9668.59    HV09    AS13  01/01/2019 13:04  Lakeisha Testerman     \n",
       "2 -291191.30    HV09    AS13  01/01/2019 13:04  Lakeisha Testerman     \n",
       "3  291191.30    HV09    AS13  01/01/2019 13:04  Lakeisha Testerman     \n",
       "4  -12848.50    DF18    TC01  30/01/2019 09:56        Jon Mckinley     \n",
       "\n",
       "   JnlAuthoriserName  \n",
       "0  Jonelle Moseley    \n",
       "1  Jonelle Moseley    \n",
       "2  Jonelle Moseley    \n",
       "3  Jonelle Moseley    \n",
       "4     Johnny Hevey    "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine your final data\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is assessed in the assessment.\n",
    "\n",
    "When dropping duplicates, we need to save the changes to our datasets in order to obtain the correct number of rows.\n",
    "Similarly, we need to save the changes when appending the journal datasets together, and when we are merging datasets to add the employee names and dropping irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3985, 14)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the size of the final journal dataset\n",
    "journals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Please take a note of the size of the final wrangled journals dataset.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select here to type your answer: 3984,14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assessment guidance\n",
    "\n",
    "You are ready to take the assessment.\n",
    "\n",
    "Remember, you should have fully completed your task and recorded your answers in Jupyter Notebook before moving on to the assessment. You can keep your Jupyter Notebook open in a separate browser window to refer to as you take the assessment.  \n",
    "\n",
    "You will receive a score following completion of the assessment. If you have scored below the target mark of 60% for the section, you are recommended to refresh your knowledge in the course content (if purchased) and rework your Jupyter Notebook before re-attempting the assessment. You have a maximum of three assessment attempts.\n",
    "\n",
    "You should aim to achieve a target score of 60% in each section of the case study. To pass the case study and be awarded the ICAEW Certificate, you are required to achieve a pass mark of 60% overall, averaged over all five sections, so do not be disheartened if you score below 60% in any one section, as a higher score in one or more of the other sections will contribute to the overall pass mark of 60%.\n",
    "\n",
    "IMPORTANT: When submitting to the assessment portal, please do not navigate away from it until you have submitted all of your answers for that task. In between any of your 3 assessment attempts (but not during an attempt) you may navigate back to the course content, if you have purchased it, to refresh your knowledge and revisit your Jupyter Notebook to rework your analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
